{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_py36",
      "language": "python",
      "name": "conda_py36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alanawd/AIDS/blob/main/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BINYnOPDhbwL"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiYwEowLhgtp"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6wrUKP7hq7Z"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "VsOgvrirhbwP"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lanTRYn0hbwR"
      },
      "source": [
        "encoded_input = tokenizer([\"Hello, I'm a single sentence!\", \"I'm a second sentence\", \"I loved the show!\"], padding=True, return_tensors='pt')\n",
        "encoded_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OXkZcmRhbwS"
      },
      "source": [
        "tokenizer.decode(encoded_input['input_ids'][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytP1W5n_hbwT"
      },
      "source": [
        "encoded_input['input_ids'][1][encoded_input['attention_mask'][1]==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qezh186uhbwT"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F9UplmShbwU"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvlZPyJqhbwU"
      },
      "source": [
        "import transformers\n",
        "from datasets import list_datasets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IPcTTxa-hbwV"
      },
      "source": [
        "datasets_list = list_datasets()\n",
        "len(datasets_list)\n",
        "print(', '.join(dataset for dataset in datasets_list))\n",
        "# https://huggingface.co/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "LPq90S1ohbwV"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('trec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te2Y21xfhbwW"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsFOiAb9hbwW"
      },
      "source": [
        "dataset['train'].shuffle()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eFhWtK5hbwX"
      },
      "source": [
        "dataset['train'].features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx6AsNvEhbwX"
      },
      "source": [
        "dataset['train'].features['label-coarse'].names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCXJgusjhbwX"
      },
      "source": [
        "dataset = dataset.remove_columns(\"label-fine\")\n",
        "dataset = dataset.rename_column('label-coarse', 'labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzumexYMhbwY"
      },
      "source": [
        "dataset['train'].features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub3ukRq4hbwY"
      },
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\")\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PYbriODhbwZ"
      },
      "source": [
        "train_dataset = tokenized_datasets['train']\n",
        "small_train_dataset = train_dataset.shuffle().select(range(500))\n",
        "eval_dataset = tokenized_datasets[\"test\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoXPBiqihbwZ"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6, n_layers=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khz29Pn9lc_Z"
      },
      "source": [
        "model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V_wnKfEkd1T"
      },
      "source": [
        "model.config.n_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQNKtDJWhbwa"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\"uni_twitter/bert_trainer/\",\n",
        "                                  evaluation_strategy=\"epoch\", \n",
        "                                  num_train_epochs = 10,\n",
        "                                  per_device_train_batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqNkdq2Qhbwa"
      },
      "source": [
        "training_args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-My2sDydhbwa"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=eval_dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isAZhmoKhbwb"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xxqdDlYhbwb"
      },
      "source": [
        "model.device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIAqszxuhbwc"
      },
      "source": [
        "sentence = 'Where are my keys?'\n",
        "tokens = tokenizer(sentence, return_tensors='pt')\n",
        "tokens\n",
        "# tokenizer.decode(tokens['input_ids']), tokens\n",
        "preds = model(tokens['input_ids'].to(model.device))[0]\n",
        "idx = preds.argmax(-1)\n",
        "\n",
        "small_train_dataset.features['labels'].names[idx]\n",
        "\n",
        "for a,b in zip(small_train_dataset.features['labels'].names, preds.softmax(-1).detach().cpu().numpy()[0]):\n",
        "    print(a, b)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7Fq5s6cXhbwe"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smq88ZVzhbwc"
      },
      "source": [
        "sentence = 'Where are my keys?'\n",
        "tokens = tokenizer(sentence, return_tensors='pt')\n",
        "tokens\n",
        "# tokenizer.decode(tokens['input_ids']), tokens\n",
        "preds = model(tokens['input_ids'].to(model.device))[0]\n",
        "idx = preds.argmax(-1)\n",
        "\n",
        "small_train_dataset.features['labels'].names[idx]\n",
        "\n",
        "for a,b in zip(small_train_dataset.features['labels'].names, preds.softmax(-1).detach().cpu().numpy()[0]):\n",
        "    print(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYtaThFEhbwf"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZdAGBi1hbwf"
      },
      "source": [
        "def get_prediction(text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    \n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    # perform inference to our model\n",
        "    outputs = model(inputs['input_ids'].to(model.device))\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return small_train_dataset.features['labels'].names[probs.argmax()], probs.max().detach().cpu().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnqIVLVxhbwg"
      },
      "source": [
        "get_prediction(['How are you?'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N8MBrbGgH37"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5co9__BOgH-Q"
      },
      "source": [
        "# GPT-2 Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSQcv2P3hbwh"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDgyk9AHhbwj"
      },
      "source": [
        "tokenized = tokenizer(\"what are you going to do?\", return_tensors='pt')\n",
        "tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "z-Za-Vvkhbwj"
      },
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id = tokenizer.eos_token_id)\n",
        "model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLL5egLhbwk"
      },
      "source": [
        "ids = tokenizer(\"The shawshank\")['input_ids']\n",
        "[tokenizer.decode(x) for x in ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb6gSa3ztd0F"
      },
      "source": [
        "text = \"I lost my keys\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "# tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXHad5HgtRJn"
      },
      "source": [
        "output = model.generate(input_ids.to(model.device), \n",
        "max_length = 10000, \n",
        "num_beams = 10,\n",
        "no_repeat_ngram_size  = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbLdYE-EyGEy"
      },
      "source": [
        "tokenizer.decode(output.squeeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}